from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import smtplib
import os
import json
from email.message import EmailMessage
from sshtunnel import SSHTunnelForwarder
import pymysql
import numpy as np
from airflow.operators.python import get_current_context


# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def run_str_script():
    config_file_path = '/opt/airflow/dags/New Config.json'
    with open(config_file_path, 'r') as f:
        config = json.load(f)

    ssh_host = config['ssh']['host']
    ssh_user = config['ssh']['user']
    ssh_password = config['ssh']['password']
    ssh_pkey = '/opt/airflow/dags/New GO-INFRAS.pem'
    sql_hostname = config['sql']['hostname']
    sql_password = config['sql']['password']
    sql_database = config['sql']['main_database']
    sql_username = config['sql']['username']
    sql_port = config['sql']['port']

    with SSHTunnelForwarder(
        (ssh_host, 22),
        ssh_username=ssh_user,
        ssh_password=ssh_password,
        ssh_pkey=ssh_pkey,
        remote_bind_address=(sql_hostname, int(sql_port))
    ) as tunnel:
        conn = pymysql.connect(
            host='127.0.0.1',
            user=sql_username,
            passwd=sql_password,
            db=sql_database,
            port=tunnel.local_bind_port
        )

        query = """
            SELECT
                rental_id AS ref_id,
                quote_id AS trx_id,
                quote_status,
                quote_num AS trx_num,
                rental_date_delivery AS trx_date,
                category_id,
                quoteitem_id AS trx_item_id,
                quoteitem_promotion_id AS promo_id,
                quoteitem_product_name,
                quoteitem_quantity AS item_qty,
                quoteitem_unitprice AS item_price,
                quoteitem_original_unitprice AS item_original_price,
                quoteitem_total AS item_total,
                quote_amount AS trx_total,
                IFNULL(tbl_provinces.province_name, '') AS province_name,
                scootaround_cruise_lines.cruiseline_name AS cruiseline_name,
                IFNULL(quote_region.region_number, customer_region.region_number) AS deptcode,
                '' AS countrycode,
                IFNULL(
                    (SELECT rentaltype_name FROM scootaround_rental_types WHERE rental_rentaltype_id = rentaltype_id),
                    ''
                ) AS channel,
                'rental' AS trx_type,
                rental_event_id AS project_id,
                tp.project_name, 
                v.vendor_name, 
                tax_amount
            FROM scootaround_rentals
            JOIN tbl_quotes ON rental_quote_id = quote_id
            JOIN tbl_quote_items ON quoteitem_quote_id = quote_id
            LEFT JOIN scootaround_rental_vendors rv ON rv.rentalvendor_rental_id = scootaround_rentals.rental_id
            LEFT JOIN tbl_vendors v ON v.vendor_id = rv.rentalvendor_vendor_id
            LEFT JOIN (
                SELECT quotetax_quote_id, SUM(quotetax_amount) AS tax_amount
                FROM tbl_quote_tax  
                GROUP BY quotetax_quote_id
            ) AS tq_tax ON quotetax_quote_id = quote_id
            LEFT JOIN tbl_products ON quoteitem_product_id = product_id
            LEFT JOIN tbl_categories ON product_category_id = category_id
            LEFT JOIN tbl_departments ON category_department_id = department_id
            JOIN tbl_customers ON quote_customerid = tbl_customers.customer_id
            LEFT JOIN tbl_provinces ON customer_province_id = province_id
            LEFT JOIN tbl_regions AS customer_region ON customer_region_id = customer_region.region_id
            LEFT JOIN tbl_regions AS quote_region ON quote_region_id = quote_region.region_id
            LEFT JOIN tbl_projects tp ON tp.project_id = scootaround_rentals.rental_event_id
            LEFT JOIN `scootaround_cruise_lines` ON (`scootaround_rentals`.`rental_cruiseline_id` = `scootaround_cruise_lines`.`cruiseline_id`)
            WHERE
                quote_status NOT IN (199, 198, 20, 9)
                AND rental_date_delivery BETWEEN '2025-01-01' AND '2026-12-31'
            GROUP BY trx_item_id;
        """

        data = pd.read_sql_query(query, conn)

        data['item_price_adjusted'] = np.where(
            data['quoteitem_product_name'].str.contains('Cancelled', case=False, na=False),
            -1 * data['item_price'],
            data['item_price']
        )
      
        data['product_type'] = np.select(
        [
        data['quoteitem_product_name'].str.contains('Cancellation', case=False, na=False) &
        ~data['quoteitem_product_name'].str.contains('without CPP|No CPP|w/CPP', case=False, na=False),

        data['quoteitem_product_name'].str.contains('EPP|Equipment Protection', case=False, na=False)
        ],
        [
        'CPP',
        'EPP'
        ],
        default='Others'
        )

        osf_vendors = [
            'Desert Botanical Gardens',
            'Palm Beach Zoo',
            'Unconventional Gift Shop - PACC',
            'UPS Office - New Orleans Ernest N. Morial Convention Center',
            'Walter E Washington Convention Center - La Colombe Coffee Shop and Convenience Store',
            'Westgate Las Vegas Resort & Casino',
            'Sheraton San Diego Hotel',
            'Westin Lake Las Vegas Resort & Spa',
            'Hotel Del Coronado',
            'Balboa Park',
            'Gaylord Pacific Resort & Convention Center'
        ]

        data['vendor_type'] = np.where(
            data['vendor_name'].isin(osf_vendors),
            'osf_vendors',
            'other_vendors'
        )
        
        # Define conditions for 'channel_group'
        conditions = [
            # 1. Channel is 'Cruise'
            data['channel'] == 'Cruise',
        
            # 2. Custom-related channels, vendor is NOT osf
            data['channel'].isin(['Custom', 'AR - Custom', 'Cruise - Custom']) & 
            (data['vendor_type'] != 'osf_vendors'),
        
            # 3. Facility/OSR channels, vendor is NOT osf
            data['channel'].isin(['AR - Facility', 'OSR - Custom', 'OSR - Facility', 'OSR']) & 
            (data['vendor_type'] != 'osf_vendors'),
        
            # 4. Facility/OSR channels, vendor is osf AND quote_status in [1, 6, 7]
            data['channel'].isin(['Custom', 'AR - Custom', 'Cruise - Custom','AR - Facility', 'OSR - Custom', 'OSR - Facility', 'OSR']) & 
            (data['vendor_type'] == 'osf_vendors') & 
            (data['quote_status'].isin([1, 6, 7]))
        ]


        # Define the corresponding values
        choices = ['Cruise', 'Land', 'Event (OSR)', 'OSF']

        # Create the new column
        data['channel_regroup'] = np.select(conditions, choices, default='TEST')


        # Create a column where only the first (trx_num, tax_amount) keeps the value; others get 0
        data['tax_amount_deduped'] = data.duplicated(subset=['trx_num', 'tax_amount'], keep='first')
      
        data['tax_adjusted'] = np.where(data['tax_amount_deduped'], 0, data['tax_amount'])      
        data.drop(columns='tax_amount_deduped', inplace=True)
      
        category_map_df = pd.read_csv(
            "/opt/airflow/dags/item_category_map.csv", 
            usecols=["Item", "Item Category"]
        )
    
    
        # Optional: standardize casing/whitespace to improve match reliability
        data['quoteitem_product_name'] = data['quoteitem_product_name'].str.strip().str.lower()
        category_map_df['Item'] = category_map_df['Item'].str.strip().str.lower()
    
        # Create a dictionary and map the item
        item_dict = dict(zip(category_map_df['Item'], category_map_df['Item Category']))
        data['Item Category'] = data['quoteitem_product_name'].map(item_dict)
    
        conditions = [
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('scooter') |
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('whill ri'),#Scooter
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('cancellation') & 
            ~data['quoteitem_product_name'].str.contains('without'),#CPP
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('cancellation') & 
            data['quoteitem_product_name'].str.contains('without') | data['quoteitem_product_name'].str.contains('missing/damaged equipment'),#Other Parts
            data['Item Category'].isna() & 
            (
                data['quoteitem_product_name'].str.contains('equipment', case=False, na=False)
                |
                data['quoteitem_product_name'].str.contains('epp', case=False, na=False)
            ) & 
            ~data['quoteitem_product_name'].str.contains('missing/damaged', case=False, na=False),#EPP
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('powerchair'),
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('ci') | 
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('c2'),
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('canopy'),
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('rollator'),
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('concentrator', case=False),
            data['Item Category'].isna() & data['quoteitem_product_name'].str.contains('recliner'),       
        ]
    
        choices = [
            'Scooter',
            'CPP',
            'Other Parts',
            'EPP',
            'Powerchair',
            'Model C2',
            'Canopy',
            'Rollator',
            'Concentrator',
            'Recliner'
        ]
    
        # Create a mask for rows where 'Item Category' is blank
        mask_na = data['Item Category'].isna()
    
        # Apply logic only to those rows
        data.loc[mask_na, 'Item Category'] = np.select(
            condlist=[cond[mask_na] for cond in conditions],
            choicelist=choices,
            default='Other Parts'
        )
      
        import pytz
        from datetime import datetime
        today = datetime.now(pytz.timezone("America/Winnipeg"))
        today_str = today.strftime('%Y-%m-%d')
        year_num = today.year
        month_num = today.month
        month_name = today.strftime('%B')  # e.g., June
        today_str = datetime.now(pytz.timezone("America/Winnipeg")).strftime('%Y-%m-%d')
        month_folder = f"{month_num}. {month_name}"

        # Final save path
        base_path = f"/host/onedrive/FQ - FP&A/Perm File/5 - Global Office Backup Reports/{year_num}"

        target_folder = os.path.join(base_path, month_folder)
        os.makedirs(target_folder, exist_ok=True)
        
        parquet_file_path = os.path.join(target_folder, f"str_rev_{today_str}.parquet")
        
        # Save to Parquet
        data.to_parquet(parquet_file_path, index=False)

        print(f" File saved to: {parquet_file_path}")

        latest_folder = "/host/onedrive/FQ - FP&A/Perm File/5 - Global Office Backup Reports/latest"

        latest_file_path = os.path.join(latest_folder, "str_rev_latest.parquet")

        # Save latest (overwrite)
        data.to_parquet(latest_file_path, index=False)

        print(f" Latest file saved to: {latest_file_path}")
        
        # --- Close connection ---
        conn.close()

        
  
from pendulum import timezone
local_tz = timezone("America/Winnipeg")

# DAG definition
with DAG(
    dag_id='save_str_report_onedrive',
    default_args=default_args,
    description='Save STR Revenue Report Daily into OneDrive',
    schedule='59 23 * * *',
    start_date=datetime(2025, 5, 5, tzinfo=local_tz),
    catchup=False,
    tags=['str', 'revenue']
) as dag:

    run_report = PythonOperator(
        task_id='run_str_report_script',
        python_callable=run_str_script
    )
