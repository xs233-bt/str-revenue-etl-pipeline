ğŸ“Œ Overview

This project implements a production-style automated ETL pipeline that extracts STR revenue data from a secure AWS RDS MySQL database, transforms it using Python, and delivers analytics-ready Parquet datasets for downstream BI consumption.

The pipeline is designed to mirror real enterprise data engineering workflows, with strong emphasis on:

Secure network access

Automated orchestration

Historical data preservation

BI-optimized outputs

ğŸ§­ High-Level Architecture

Daily automated flow:

Airflow DAG
   â†“
Python SSH Tunnel (Bastion)
   â†“
AWS RDS MySQL
   â†“
Python Transform (Pandas / PyArrow)
   â†“
Parquet Storage (OneDrive)
   â†“
Power BI Dashboards

ğŸ—ï¸ Architecture Breakdown
1ï¸âƒ£ Airflow DAG â€” Orchestration Layer

Runs on a daily schedule

Controls task sequencing, retries, and failure handling

Serves as the single entry point for the entire pipeline

Key responsibilities

Establish secure database access

Trigger extraction & transformation tasks

Ensure pipeline reliability and observability

2ï¸âƒ£ Secure SSH Tunnel â€” Network & Security Layer

Uses an EC2 Jump Host (Bastion)

Establishes an SSH tunnel programmatically via Python:

sshtunnel

paramiko

Forwards:

Local port â†’ AWS RDS MySQL : 3306

Why this design

RDS is not publicly accessible

No inbound database exposure

Aligns with enterprise security best practices

3ï¸âƒ£ AWS RDS MySQL â€” Source System

Acts as the system of record for STR revenue

Contains transactional operational data

Accessed only through the SSH tunnel

4ï¸âƒ£ Python Transform Layer â€” ELT Core

Extracts data via the forwarded local port

Transforms data using:

Pandas for data manipulation

PyArrow for Parquet serialization

Transform responsibilities

Data cleaning & type normalization

Metric standardization

Schema stabilization for BI tools

5ï¸âƒ£ Parquet Storage â€” Analytics Layer

Data is written in Parquet format and organized into two logical zones:

ğŸ“‚ Historical

Immutable snapshots of all STR data

Supports:

Backfills

Audits

Reprocessing

ğŸ“‚ Current

Latest daily dataset only

Optimized for:

Fast BI refresh

Low query overhead

Why Parquet

Columnar format

Efficient storage

Native compatibility with Power BI & modern analytics tools

6ï¸âƒ£ BI Consumption â€” Power BI

Power BI reads Parquet outputs directly

Enables:

Daily revenue dashboards

Trend analysis

Operational monitoring

ğŸ“ Repository Structure
str-revenue-etl-pipeline/
â”œâ”€â”€ dags/                     # Airflow DAG definitions
â”œâ”€â”€ src/                      # ETL & transformation logic
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ parquet_to_csv/       # Stakeholder utility tool
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ str_revenue_etl_architecture.png
â”œâ”€â”€ README.md

ğŸ› ï¸ Tech Stack

Apache Airflow â€” Orchestration

Python â€” Core ETL logic

Pandas / PyArrow â€” Transformation & Parquet output

AWS EC2 â€” SSH Bastion host

AWS RDS MySQL â€” Source database

OneDrive â€” Analytics storage layer

Power BI â€” Visualization & reporting

ğŸ§  Key Engineering Decisions
Decision	Rationale
SSH Bastion	Secure access without public DB exposure
Parquet Output	BI-optimized, cost-efficient storage
Historical + Current split	Supports audit & fast dashboards
Airflow Orchestration	Production-grade scheduling & retries
Python-based ELT	Flexibility and testability
ğŸ“ˆ Why This Project Matters (Portfolio Perspective)

This project demonstrates:

Real-world enterprise security patterns

End-to-end production ETL ownership

Strong understanding of ELT vs ETL

Analytics-driven data modeling

Stakeholder-friendly data delivery

ğŸ§ª Future Enhancements

Data quality checks (Great Expectations)

Schema evolution handling

Iceberg / Delta Lake integration

Metadata & data lineage tracking

Automated alerting on data anomalies
